{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67969ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import  word_tokenize\n",
    "import re\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import nn,optim\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93a4cbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the data\n",
    "train_path = r\"C:\\Ashvin\\AI ML\\Project\\Comment Toxicity\\train.csv\"\n",
    "test_path = r\"C:\\Ashvin\\AI ML\\Project\\Comment Toxicity\\test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3a5d2cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               0\n",
       "comment_text     0\n",
       "toxic            0\n",
       "severe_toxic     0\n",
       "obscene          0\n",
       "threat           0\n",
       "insult           0\n",
       "identity_hate    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for null values\n",
    "train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49f0c4bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id              0\n",
       "comment_text    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for null values\n",
    "test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3499737b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca86ade4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2383eecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(['id'],axis=1,inplace=True)\n",
    "test_df.drop(['id'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e05b9c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text  toxic  \\\n",
       "0       Explanation\\nWhy the edits made under my usern...      0   \n",
       "1       D'aww! He matches this background colour I'm s...      0   \n",
       "2       Hey man, I'm really not trying to edit war. It...      0   \n",
       "3       \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4       You, sir, are my hero. Any chance you remember...      0   \n",
       "...                                                   ...    ...   \n",
       "159566  \":::::And for the second time of asking, when ...      0   \n",
       "159567  You should be ashamed of yourself \\n\\nThat is ...      0   \n",
       "159568  Spitzer \\n\\nUmm, theres no actual article for ...      0   \n",
       "159569  And it looks like it was actually you who put ...      0   \n",
       "159570  \"\\nAnd ... I really don't think you understand...      0   \n",
       "\n",
       "        severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0                  0        0       0       0              0  \n",
       "1                  0        0       0       0              0  \n",
       "2                  0        0       0       0              0  \n",
       "3                  0        0       0       0              0  \n",
       "4                  0        0       0       0              0  \n",
       "...              ...      ...     ...     ...            ...  \n",
       "159566             0        0       0       0              0  \n",
       "159567             0        0       0       0              0  \n",
       "159568             0        0       0       0              0  \n",
       "159569             0        0       0       0              0  \n",
       "159570             0        0       0       0              0  \n",
       "\n",
       "[159571 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cb36a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Pre processing \n",
    "# Part 1. Stopwords removal, cleaning text, tokenization\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if (t.isalpha()) and (t not in sw)]\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens if len(t)>=4]\n",
    "    return tokens\n",
    "\n",
    "#applying the clean text on it\n",
    "train_df['cleaned'] = train_df['comment_text'].apply(get_clean_text)\n",
    "test_df['cleaned'] = test_df['comment_text'].apply(get_clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7459abe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2. Word2idx , vector , padding\n",
    "# Create word to index --> convert word into numeric IDs so it can be used as input to newural network\n",
    "\n",
    "X_train = train_df['cleaned']\n",
    "X_test = test_df['cleaned']\n",
    "\n",
    "word2idx = {\"<PAD>\":0,\"<UNK>\":1}\n",
    "idx = 2\n",
    "for tokens in X_train:\n",
    "    for word in tokens:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = idx\n",
    "            idx += 1\n",
    "\n",
    "#Vectorization -> Turning numeric ids to vectors\n",
    "train_vector = []\n",
    "\n",
    "for sent in X_train.values:\n",
    "    inner_vector = []\n",
    "    for word in sent:\n",
    "        inner_vector.append(word2idx.get(word,word2idx[\"<UNK>\"]))\n",
    "    train_vector.append(torch.tensor(inner_vector))\n",
    "\n",
    "test_vector = []\n",
    "\n",
    "for sent in X_test.values:\n",
    "    inner_vector = []\n",
    "    for word in sent:\n",
    "        inner_vector.append(word2idx.get(word,word2idx[\"<UNK>\"]))\n",
    "    test_vector.append(torch.tensor(inner_vector))\n",
    "\n",
    "#Padding --> ensures all the tensors have equal lengths \n",
    "train_padded = pad_sequence(train_vector,batch_first=True,padding_value=0)\n",
    "test_padded = pad_sequence(test_vector,batch_first=True,padding_value=0)\n",
    "\n",
    "# convert the remaining columns to torch and defining the target that these are the output columns\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train_labels = torch.tensor(train_df[label_cols].values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "802135c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([159571, 6])\n",
      "torch.Size([159571, 1000])\n"
     ]
    }
   ],
   "source": [
    "print(train_labels.shape)\n",
    "print(train_padded.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f822fc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self,vocab_size,emb_dim,hidden_dim,output_dim):\n",
    "        super(BiLSTM,self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,emb_dim,padding_idx=0)\n",
    "        self.lstm = nn.LSTM(emb_dim,hidden_dim,bidirectional=True,batch_first=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim*2,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim,output_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self,X):\n",
    "        X = self.embedding(X)\n",
    "        output,(hidden,cell_state) = self.lstm(X)\n",
    "        hidden_forward = hidden[-2, :, :]\n",
    "        hidden_backward = hidden[-1, :, :]\n",
    "        concatenated = torch.cat((hidden_forward,hidden_backward),dim=1)\n",
    "        output = self.classifier(concatenated)\n",
    "        return output \n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, output_dim, kernel_sizes=[3,4,5], num_filters=100):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, num_filters, (k, emb_dim)) for k in kernel_sizes])\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * num_filters, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # (batch_size, seq_len, emb_dim)\n",
    "        x = x.unsqueeze(1)     # (batch_size, 1, seq_len, emb_dim)\n",
    "        x = [torch.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
    "        x = [torch.max(pool, dim=2)[0] for pool in x]\n",
    "        x = torch.cat(x, dim=1)\n",
    "        x = self.dropout(x)\n",
    "        return self.sigmoid(self.fc(x))\n",
    "\n",
    "vocab_size = len(word2idx)\n",
    "emb_dim = 64\n",
    "hidden_dim = 32\n",
    "output_dim = len(label_cols)\n",
    "\n",
    "models = [BiLSTM(vocab_size,emb_dim,hidden_dim,output_dim),\n",
    "          TextCNN(vocab_size, emb_dim, output_dim)]\n",
    "\n",
    "input_data = train_padded\n",
    "labels = train_labels\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Create dataset and loader\n",
    "dataset = TensorDataset(input_data, labels)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "for model in models:\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    print(f\"\\nTraining model: {type(model).__name__}\")\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(batch_x)\n",
    "            loss = criterion(preds, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model(input_data)\n",
    "        prediction = (prediction>0.5).float()\n",
    "        print(f\"Accuracy: {accuracy_score(labels.numpy(),prediction.numpy())}\")\n",
    "        print(f\"Precision: {precision_score(labels.numpy(),prediction.numpy(),average='macro', zero_division=0)}\")\n",
    "        print(f\"Recall : {recall_score(labels.numpy(),prediction.numpy(),average='macro', zero_division=0)}\")\n",
    "        print(f\"F1 Score: {f1_score(labels.numpy(),prediction.numpy(),average='macro', zero_division=0)}\")\n",
    "    model_path = f\"{type(model).__name__}_model.pth\"\n",
    "    torch.save(model.state_dict(),model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4d653c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Training model: BiLSTM\n",
      "Epoch [1/15], Loss: 0.7409\n",
      "Epoch [2/15], Loss: 0.5036\n",
      "Epoch [3/15], Loss: 0.3704\n",
      "Epoch [4/15], Loss: 0.2945\n",
      "Epoch [5/15], Loss: 0.2440\n",
      "Epoch [6/15], Loss: 0.2030\n",
      "Epoch [7/15], Loss: 0.1729\n",
      "Epoch [8/15], Loss: 0.1515\n",
      "Epoch [9/15], Loss: 0.1327\n",
      "Epoch [10/15], Loss: 0.1374\n",
      "Epoch [11/15], Loss: 0.1433\n",
      "Epoch [12/15], Loss: 0.1318\n",
      "Epoch [13/15], Loss: 0.0987\n",
      "Epoch [14/15], Loss: 0.0898\n",
      "Epoch [15/15], Loss: 0.0797\n",
      "Accuracy: 0.9322746614359753\n",
      "Precision: 0.5482897137211845\n",
      "Recall : 0.9987014954472268\n",
      "F1 Score: 0.6904724204027296\n",
      "Metrics saved to BiLSTM_metrics.json\n",
      "\n",
      "Training model: TextCNN\n",
      "Epoch [1/15], Loss: 0.7480\n",
      "Epoch [2/15], Loss: 0.4590\n",
      "Epoch [3/15], Loss: 0.3713\n",
      "Epoch [4/15], Loss: 0.3197\n",
      "Epoch [5/15], Loss: 0.2940\n",
      "Epoch [6/15], Loss: 0.2674\n",
      "Epoch [7/15], Loss: 0.2510\n",
      "Epoch [8/15], Loss: 0.2374\n",
      "Epoch [9/15], Loss: 0.2236\n",
      "Epoch [10/15], Loss: 0.2117\n",
      "Epoch [11/15], Loss: 0.2014\n",
      "Epoch [12/15], Loss: 0.1880\n",
      "Epoch [13/15], Loss: 0.1838\n",
      "Epoch [14/15], Loss: 0.1744\n",
      "Epoch [15/15], Loss: 0.1708\n",
      "Accuracy: 0.8949495835709496\n",
      "Precision: 0.4435334576555454\n",
      "Recall : 0.993140785699921\n",
      "F1 Score: 0.5797171397327897\n",
      "Metrics saved to TextCNN_metrics.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import torch\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import  word_tokenize\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "from torch import nn,optim\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "import json\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# importing the data\n",
    "train_path = r\"C:\\Ashvin\\AI ML\\Project\\Comment Toxicity\\train.csv\"\n",
    "test_path = r\"C:\\Ashvin\\AI ML\\Project\\Comment Toxicity\\test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "train_df.drop(['id'],axis=1,inplace=True)\n",
    "test_df.drop(['id'],axis=1,inplace=True)\n",
    "\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if (t.isalpha()) and (t not in sw)]\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens if len(t)>=4]\n",
    "    return tokens\n",
    "\n",
    "# applying the clean text on it\n",
    "train_df['cleaned'] = train_df['comment_text'].apply(get_clean_text)\n",
    "test_df['cleaned'] = test_df['comment_text'].apply(get_clean_text)\n",
    "\n",
    "\n",
    "# Part 2. Word2idx , vector , padding\n",
    "# Create word to index --> convert word into numeric IDs so it can be used as input to newural network\n",
    "\n",
    "X_train = train_df['cleaned']\n",
    "X_test = test_df['cleaned']\n",
    "\n",
    "word2idx = {\"<PAD>\":0,\"<UNK>\":1}\n",
    "idx = 2\n",
    "for tokens in X_train:\n",
    "    for word in tokens:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = idx\n",
    "            idx += 1\n",
    "\n",
    "\n",
    "# Vectorization -> Turning numeric ids to vectors\n",
    "train_vector = []\n",
    "\n",
    "for sent in X_train.values:\n",
    "    inner_vector = []\n",
    "    for word in sent:\n",
    "        inner_vector.append(word2idx.get(word,word2idx[\"<UNK>\"]))\n",
    "    train_vector.append(torch.tensor(inner_vector))\n",
    "\n",
    "test_vector = []\n",
    "\n",
    "for sent in X_test.values:\n",
    "    inner_vector = []\n",
    "    for word in sent:\n",
    "        inner_vector.append(word2idx.get(word,word2idx[\"<UNK>\"]))\n",
    "    test_vector.append(torch.tensor(inner_vector))\n",
    "\n",
    "# Padding --> ensures all the tensors have equal lengths \n",
    "train_padded = pad_sequence(train_vector,batch_first=True,padding_value=0)\n",
    "test_padded = pad_sequence(test_vector,batch_first=True,padding_value=0)\n",
    "\n",
    "# convert the remaining columns to torch and defining the target that these are the output columns\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train_labels = torch.tensor(train_df[label_cols].values, dtype=torch.float32)\n",
    "\n",
    "# Pos weight for imbalance\n",
    "# Compute pos_weight = (num_neg / num_pos) per class\n",
    "num_pos = train_df[label_cols].sum(axis=0).values\n",
    "num_neg = len(train_df) - num_pos\n",
    "pos_weight = torch.tensor(num_neg / (num_pos+1e-5), dtype=torch.float32).to(device)\n",
    "\n",
    "# Create dataset and loader\n",
    "batch_size = 32\n",
    "dataset = TensorDataset(train_padded, train_labels)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "# Architecture\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self,vocab_size,emb_dim,hidden_dim,output_dim):\n",
    "        super(BiLSTM,self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,emb_dim,padding_idx=0)\n",
    "        self.lstm = nn.LSTM(emb_dim,hidden_dim,bidirectional=True,batch_first=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim*2,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim,output_dim),\n",
    "        )\n",
    "    def forward(self,X):\n",
    "        X = self.embedding(X)\n",
    "        output,(hidden,cell_state) = self.lstm(X)\n",
    "        hidden_forward = hidden[-2, :, :]\n",
    "        hidden_backward = hidden[-1, :, :]\n",
    "        concatenated = torch.cat((hidden_forward,hidden_backward),dim=1)\n",
    "        output = self.classifier(concatenated)\n",
    "        return output \n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, output_dim, kernel_sizes=[3,4,5], num_filters=100):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, num_filters, (k, emb_dim)) for k in kernel_sizes])\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * num_filters, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # (batch_size, seq_len, emb_dim)\n",
    "        x = x.unsqueeze(1)     # (batch_size, 1, seq_len, emb_dim)\n",
    "        x = [torch.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
    "        x = [torch.max(pool, dim=2)[0] for pool in x]\n",
    "        x = torch.cat(x, dim=1)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Train & evaluate\n",
    "vocab_size = len(word2idx)\n",
    "emb_dim =  192\n",
    "hidden_dim = 96\n",
    "output_dim = len(label_cols)\n",
    "\n",
    "models = [BiLSTM(vocab_size,emb_dim,hidden_dim,output_dim).to(device),\n",
    "          TextCNN(vocab_size, emb_dim, output_dim).to(device)]\n",
    "\n",
    "input_train = train_padded.to(device)\n",
    "labels = train_labels.to(device)\n",
    "\n",
    "epochs = 15\n",
    "\n",
    "for model in models:\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0003)\n",
    "\n",
    "    print(f\"\\nTraining model: {type(model).__name__}\")\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch_x)\n",
    "            loss = criterion(logits, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluate in batches to avoid GPU OOM\n",
    "\n",
    "    model.eval()\n",
    "    train_eval_loader = DataLoader(TensorDataset(input_train, labels), batch_size=64, shuffle=False)\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch_x, _ in train_eval_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            logits = model(batch_x)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs>0.5).float()\n",
    "            all_preds.append(preds.cpu())\n",
    "\n",
    "    pred_train = torch.cat(all_preds, dim=0).numpy()\n",
    "    labels_cpu = labels.cpu().numpy()\n",
    "\n",
    "    print(f\"Accuracy: {accuracy_score(labels_cpu,pred_train)}\")\n",
    "    print(f\"Precision: {precision_score(labels_cpu,pred_train,average='macro', zero_division=0)}\")\n",
    "    print(f\"Recall : {recall_score(labels_cpu,pred_train,average='macro', zero_division=0)}\")\n",
    "    print(f\"F1 Score: {f1_score(labels_cpu,pred_train,average='macro', zero_division=0)}\")\n",
    "\n",
    "    model_path = f\"{type(model).__name__}_model.pth\"\n",
    "    torch.save(model.state_dict(),model_path)\n",
    "\n",
    "    metrics = {\n",
    "        \"Accuracy\": float(accuracy_score(labels_cpu, pred_train)),\n",
    "        \"Precision\": float(precision_score(labels_cpu, pred_train, average='macro', zero_division=0)),\n",
    "        \"Recall\": float(recall_score(labels_cpu, pred_train, average='macro', zero_division=0)),\n",
    "        \"F1 Score\": float(f1_score(labels_cpu, pred_train, average='macro', zero_division=0))\n",
    "    }\n",
    "\n",
    "    metrics_filename = f\"{type(model).__name__}_metrics.json\"\n",
    "    with open(metrics_filename, 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    print(f\"Metrics saved to {metrics_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d5ce64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using device: cuda\n",
    "\n",
    "# Training model: BiLSTM\n",
    "# Epoch [1/15], Loss: 0.7409\n",
    "# Epoch [2/15], Loss: 0.5036\n",
    "# Epoch [3/15], Loss: 0.3704\n",
    "# Epoch [4/15], Loss: 0.2945\n",
    "# Epoch [5/15], Loss: 0.2440\n",
    "# Epoch [6/15], Loss: 0.2030\n",
    "# Epoch [7/15], Loss: 0.1729\n",
    "# Epoch [8/15], Loss: 0.1515\n",
    "# Epoch [9/15], Loss: 0.1327\n",
    "# Epoch [10/15], Loss: 0.1374\n",
    "# Epoch [11/15], Loss: 0.1433\n",
    "# Epoch [12/15], Loss: 0.1318\n",
    "# Epoch [13/15], Loss: 0.0987\n",
    "# Epoch [14/15], Loss: 0.0898\n",
    "# Epoch [15/15], Loss: 0.0797\n",
    "# Accuracy: 0.9322746614359753\n",
    "# Precision: 0.5482897137211845\n",
    "# Recall : 0.9987014954472268\n",
    "# F1 Score: 0.6904724204027296\n",
    "# Metrics saved to BiLSTM_metrics.json\n",
    "\n",
    "# Training model: TextCNN\n",
    "# Epoch [1/15], Loss: 0.7480\n",
    "# Epoch [2/15], Loss: 0.4590\n",
    "# Epoch [3/15], Loss: 0.3713\n",
    "# Epoch [4/15], Loss: 0.3197\n",
    "# Epoch [5/15], Loss: 0.2940\n",
    "# Epoch [6/15], Loss: 0.2674\n",
    "# Epoch [7/15], Loss: 0.2510\n",
    "# Epoch [8/15], Loss: 0.2374\n",
    "# Epoch [9/15], Loss: 0.2236\n",
    "# Epoch [10/15], Loss: 0.2117\n",
    "# Epoch [11/15], Loss: 0.2014\n",
    "# Epoch [12/15], Loss: 0.1880\n",
    "# Epoch [13/15], Loss: 0.1838\n",
    "# Epoch [14/15], Loss: 0.1744\n",
    "# Epoch [15/15], Loss: 0.1708\n",
    "# Accuracy: 0.8949495835709496\n",
    "# Precision: 0.4435334576555454\n",
    "# Recall : 0.993140785699921\n",
    "# F1 Score: 0.5797171397327897\n",
    "# Metrics saved to TextCNN_metrics.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7130202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import nltk\n",
    "# import re\n",
    "# import torch\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# import numpy as np\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "# from torch import nn, optim\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# import json\n",
    "\n",
    "# # -------------------------------\n",
    "# # 1️⃣ Setup\n",
    "# # -------------------------------\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# # Paths\n",
    "# train_path = r\"C:\\Ashvin\\AI ML\\Project\\Comment Toxicity\\train.csv\"\n",
    "# test_path  = r\"C:\\Ashvin\\AI ML\\Project\\Comment Toxicity\\test.csv\"\n",
    "\n",
    "# # Load data\n",
    "# train_df = pd.read_csv(train_path)\n",
    "# test_df  = pd.read_csv(test_path)\n",
    "\n",
    "# # Drop 'id' column\n",
    "# train_df.drop(['id'], axis=1, inplace=True)\n",
    "# test_df.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "# # NLP tools\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# sw = set(stopwords.words(\"english\"))\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# # -------------------------------\n",
    "# # 2️⃣ Text cleaning\n",
    "# # -------------------------------\n",
    "# def get_clean_text(text):\n",
    "#     text = text.lower()\n",
    "#     text = re.sub(r\"what's\", \"what is \", text)\n",
    "#     text = re.sub(r\"\\'s\", \" \", text)\n",
    "#     text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "#     text = re.sub(r\"can't\", \"can not \", text)\n",
    "#     text = re.sub(r\"n't\", \" not \", text)\n",
    "#     text = re.sub(r\"i'm\", \"i am \", text)\n",
    "#     text = re.sub(r\"\\'re\", \" are \", text)\n",
    "#     text = re.sub(r\"\\'d\", \" would \", text)\n",
    "#     text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "#     text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "#     tokens = word_tokenize(text)\n",
    "#     tokens = [lemmatizer.lemmatize(t) for t in tokens if t.isalpha() and t not in sw and len(t) >= 4]\n",
    "#     return tokens\n",
    "\n",
    "# train_df['cleaned'] = train_df['comment_text'].apply(get_clean_text)\n",
    "# test_df['cleaned']  = test_df['comment_text'].apply(get_clean_text)\n",
    "\n",
    "# # -------------------------------\n",
    "# # 3️⃣ Vocabulary and vectorization\n",
    "# # -------------------------------\n",
    "# word2idx = {\"<PAD>\":0, \"<UNK>\":1}\n",
    "# idx = 2\n",
    "# for tokens in train_df['cleaned']:\n",
    "#     for word in tokens:\n",
    "#         if word not in word2idx:\n",
    "#             word2idx[word] = idx\n",
    "#             idx += 1\n",
    "\n",
    "# # Vectorize\n",
    "# def vectorize(texts):\n",
    "#     return [torch.tensor([word2idx.get(word, word2idx[\"<UNK>\"]) for word in tokens]) for tokens in texts]\n",
    "\n",
    "# train_vectors = vectorize(train_df['cleaned'])\n",
    "# test_vectors  = vectorize(test_df['cleaned'])\n",
    "\n",
    "# # Pad\n",
    "# train_padded = pad_sequence(train_vectors, batch_first=True, padding_value=0)\n",
    "# test_padded  = pad_sequence(test_vectors, batch_first=True, padding_value=0)\n",
    "\n",
    "# # -------------------------------\n",
    "# # 4️⃣ Prepare labels & DataLoader\n",
    "# # -------------------------------\n",
    "# label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "# train_labels = torch.tensor(train_df[label_cols].values, dtype=torch.float32)\n",
    "\n",
    "# # Pos weight for imbalance\n",
    "# num_pos = train_df[label_cols].sum(axis=0).values\n",
    "# num_neg = len(train_df) - num_pos\n",
    "# pos_weight = torch.tensor(num_neg / (num_pos+1e-5), dtype=torch.float32).to(device)\n",
    "\n",
    "# batch_size = 32\n",
    "# dataset = TensorDataset(train_padded, train_labels)\n",
    "# train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # -------------------------------\n",
    "# # 5️⃣ Model definitions\n",
    "# # -------------------------------\n",
    "# class BiLSTM(nn.Module):\n",
    "#     def __init__(self, vocab_size, emb_dim, hidden_dim, output_dim):\n",
    "#         super(BiLSTM, self).__init__()\n",
    "#         self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "#         self.lstm = nn.LSTM(emb_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(hidden_dim*2, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(hidden_dim, output_dim)\n",
    "#         )\n",
    "#     def forward(self, x):\n",
    "#         x = self.embedding(x)\n",
    "#         _, (hidden, _) = self.lstm(x)\n",
    "#         concat = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
    "#         return self.classifier(concat)\n",
    "\n",
    "# class TextCNN(nn.Module):\n",
    "#     def __init__(self, vocab_size, emb_dim, output_dim, kernel_sizes=[3,4,5], num_filters=100):\n",
    "#         super(TextCNN, self).__init__()\n",
    "#         self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "#         self.convs = nn.ModuleList([nn.Conv2d(1, num_filters, (k, emb_dim)) for k in kernel_sizes])\n",
    "#         self.dropout = nn.Dropout(0.5)\n",
    "#         self.fc = nn.Linear(len(kernel_sizes)*num_filters, output_dim)\n",
    "#     def forward(self, x):\n",
    "#         x = self.embedding(x).unsqueeze(1)\n",
    "#         x = [torch.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
    "#         x = [torch.max(pool, dim=2)[0] for pool in x]\n",
    "#         x = torch.cat(x, dim=1)\n",
    "#         return self.fc(self.dropout(x))\n",
    "\n",
    "# # -------------------------------\n",
    "# # 6️⃣ Train & evaluate\n",
    "# # -------------------------------\n",
    "# vocab_size = len(word2idx)\n",
    "# emb_dim, hidden_dim, output_dim = 192, 96, len(label_cols)\n",
    "# epochs = 15\n",
    "# models = [\n",
    "#     BiLSTM(vocab_size, emb_dim, hidden_dim, output_dim).to(device),\n",
    "#     TextCNN(vocab_size, emb_dim, output_dim).to(device)\n",
    "# ]\n",
    "\n",
    "# input_train = train_padded.to(device)\n",
    "# labels = train_labels.to(device)\n",
    "\n",
    "# for model in models:\n",
    "#     print(f\"\\nTraining model: {type(model).__name__}\")\n",
    "#     criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=0.0003)\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         total_loss = 0\n",
    "#         for batch_x, batch_y in train_loader:\n",
    "#             batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             logits = model(batch_x)\n",
    "#             loss = criterion(logits, batch_y)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             total_loss += loss.item()\n",
    "#         print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "#     # Evaluation\n",
    "#     model.eval()\n",
    "#     preds_all = []\n",
    "#     eval_loader = DataLoader(TensorDataset(input_train, labels), batch_size=64)\n",
    "#     with torch.no_grad():\n",
    "#         for batch_x, _ in eval_loader:\n",
    "#             batch_x = batch_x.to(device)\n",
    "#             logits = model(batch_x)\n",
    "#             preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "#             preds_all.append(preds.cpu())\n",
    "#     preds = torch.cat(preds_all).numpy()\n",
    "#     labels_np = labels.cpu().numpy()\n",
    "\n",
    "#     acc = accuracy_score(labels_np, preds)\n",
    "#     prec = precision_score(labels_np, preds, average='macro', zero_division=0)\n",
    "#     rec = recall_score(labels_np, preds, average='macro', zero_division=0)\n",
    "#     f1 = f1_score(labels_np, preds, average='macro', zero_division=0)\n",
    "\n",
    "#     print(f\"Accuracy: {acc:.4f}\")\n",
    "#     print(f\"Precision: {prec:.4f}\")\n",
    "#     print(f\"Recall : {rec:.4f}\")\n",
    "#     print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "#     # Save\n",
    "#     torch.save(model.state_dict(), f\"{type(model).__name__}_model.pth\")\n",
    "#     metrics = {\"Accuracy\": acc, \"Precision\": prec, \"Recall\": rec, \"F1 Score\": f1}\n",
    "#     with open(f\"{type(model).__name__}_metrics.json\", 'w') as f:\n",
    "#         json.dump(metrics, f, indent=4)\n",
    "#     print(f\"✅ Metrics saved to {type(model).__name__}_metrics.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644e01cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #code block with functions\n",
    "# import pandas as pd\n",
    "# import re\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import random\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "# from torch import nn, optim\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# import json\n",
    "\n",
    "# # Set random seeds for reproducibility\n",
    "# SEED = 42\n",
    "# random.seed(SEED)\n",
    "# np.random.seed(SEED)\n",
    "# torch.manual_seed(SEED)\n",
    "# torch.cuda.manual_seed_all(SEED)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# # Preprocessing function\n",
    "# def get_clean_text(text, stopwords_list, lemmatizer):\n",
    "#     text = text.lower()\n",
    "#     text = re.sub(r\"what's\", \"what is \", text)\n",
    "#     text = re.sub(r\"\\'s\", \" \", text)\n",
    "#     text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "#     text = re.sub(r\"can't\", \"can not \", text)\n",
    "#     text = re.sub(r\"n't\", \" not \", text)\n",
    "#     text = re.sub(r\"i'm\", \"i am \", text)\n",
    "#     text = re.sub(r\"\\'re\", \" are \", text)\n",
    "#     text = re.sub(r\"\\'d\", \" would \", text)\n",
    "#     text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "#     text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "#     tokens = word_tokenize(text)\n",
    "#     tokens = [lemmatizer.lemmatize(t) for t in tokens if t.isalpha() and t not in stopwords_list and len(t) >= 4]\n",
    "#     return tokens\n",
    "\n",
    "# # Load and preprocess data\n",
    "# def load_and_preprocess_data(train_path, test_path):\n",
    "#     sw = stopwords.words(\"english\")\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#     train_df = pd.read_csv(train_path)\n",
    "#     test_df = pd.read_csv(test_path)\n",
    "\n",
    "#     train_df.drop(['id'], axis=1, inplace=True)\n",
    "#     test_df.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "#     train_df['cleaned'] = train_df['comment_text'].apply(lambda x: get_clean_text(x, sw, lemmatizer))\n",
    "#     test_df['cleaned'] = test_df['comment_text'].apply(lambda x: get_clean_text(x, sw, lemmatizer))\n",
    "#     return train_df, test_df\n",
    "\n",
    "# # Build vocabulary\n",
    "# def build_vocab(cleaned_texts):\n",
    "#     word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "#     idx = 2\n",
    "#     for tokens in cleaned_texts:\n",
    "#         for word in tokens:\n",
    "#             if word not in word2idx:\n",
    "#                 word2idx[word] = idx\n",
    "#                 idx += 1\n",
    "#     return word2idx\n",
    "\n",
    "# # Vectorize and pad sequences\n",
    "# def vectorize_and_pad(texts, word2idx):\n",
    "#     vectorized = []\n",
    "#     for tokens in texts:\n",
    "#         vectorized.append(torch.tensor([word2idx.get(word, word2idx[\"<UNK>\"]) for word in tokens]))\n",
    "#     return pad_sequence(vectorized, batch_first=True, padding_value=0)\n",
    "\n",
    "# # Save metrics to JSON\n",
    "# def save_metrics(metrics, filename):\n",
    "#     with open(filename, \"w\") as f:\n",
    "#         json.dump(metrics, f, indent=4)\n",
    "\n",
    "# # Train model\n",
    "# def train_model(model, train_loader, criterion, optimizer, epochs=5):\n",
    "#     model.to(device)\n",
    "#     model.train()\n",
    "#     for epoch in range(epochs):\n",
    "#         epoch_loss = 0\n",
    "#         for batch_x, batch_y in train_loader:\n",
    "#             batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             logits = model(batch_x)\n",
    "#             loss = criterion(logits, batch_y)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             epoch_loss += loss.item()\n",
    "#         print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# # Evaluate model\n",
    "# def evaluate_model(model, eval_loader, true_labels):\n",
    "#     model.eval()\n",
    "#     preds_all = []\n",
    "#     with torch.no_grad():\n",
    "#         for batch_x, _ in eval_loader:\n",
    "#             batch_x = batch_x.to(device)\n",
    "#             logits = model(batch_x)\n",
    "#             probs = torch.sigmoid(logits)\n",
    "#             preds = (probs > 0.5).float()\n",
    "#             preds_all.append(preds.cpu())\n",
    "#     pred_train = torch.cat(preds_all, dim=0).numpy()\n",
    "#     true_np = true_labels.cpu().numpy()\n",
    "#     return {\n",
    "#         \"Accuracy\": accuracy_score(true_np, pred_train),\n",
    "#         \"Precision\": precision_score(true_np, pred_train, average='macro', zero_division=0),\n",
    "#         \"Recall\": recall_score(true_np, pred_train, average='macro', zero_division=0),\n",
    "#         \"F1 Score\": f1_score(true_np, pred_train, average='macro', zero_division=0)\n",
    "#     }\n",
    "\n",
    "# # Model classes\n",
    "# class BiLSTM(nn.Module):\n",
    "#     def __init__(self, vocab_size, emb_dim, hidden_dim, output_dim):\n",
    "#         super(BiLSTM, self).__init__()\n",
    "#         self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "#         self.lstm = nn.LSTM(emb_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(hidden_dim, output_dim),\n",
    "#         )\n",
    "#     def forward(self, x):\n",
    "#         x = self.embedding(x)\n",
    "#         _, (hidden, _) = self.lstm(x)\n",
    "#         concat = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
    "#         return self.classifier(concat)\n",
    "\n",
    "# class TextCNN(nn.Module):\n",
    "#     def __init__(self, vocab_size, emb_dim, output_dim, kernel_sizes=[3,4,5], num_filters=100):\n",
    "#         super(TextCNN, self).__init__()\n",
    "#         self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "#         self.convs = nn.ModuleList([nn.Conv2d(1, num_filters, (k, emb_dim)) for k in kernel_sizes])\n",
    "#         self.dropout = nn.Dropout(0.5)\n",
    "#         self.fc = nn.Linear(len(kernel_sizes) * num_filters, output_dim)\n",
    "#     def forward(self, x):\n",
    "#         x = self.embedding(x).unsqueeze(1)\n",
    "#         x = [torch.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
    "#         x = [torch.max(pool, dim=2)[0] for pool in x]\n",
    "#         x = torch.cat(x, dim=1)\n",
    "#         return self.fc(self.dropout(x))\n",
    "\n",
    "# # Main script\n",
    "# def main():\n",
    "#     train_path = r\"C:\\Ashvin\\AI ML\\Project\\Comment Toxicity\\train.csv\"\n",
    "#     test_path = r\"C:\\Ashvin\\AI ML\\Project\\Comment Toxicity\\test.csv\"\n",
    "#     label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "#     train_df, test_df = load_and_preprocess_data(train_path, test_path)\n",
    "#     word2idx = build_vocab(train_df['cleaned'])\n",
    "#     train_padded = vectorize_and_pad(train_df['cleaned'], word2idx)\n",
    "#     train_labels = torch.tensor(train_df[label_cols].values, dtype=torch.float32)\n",
    "\n",
    "#     pos_weight = torch.tensor((len(train_df) - train_df[label_cols].sum()) / (train_df[label_cols].sum() + 1e-5), dtype=torch.float32).to(device)\n",
    "#     dataset = TensorDataset(train_padded, train_labels)\n",
    "#     train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "#     vocab_size = len(word2idx)\n",
    "#     models = {\n",
    "#         'BiLSTM': BiLSTM(vocab_size, 128, 80, len(label_cols)),\n",
    "#         'TextCNN': TextCNN(vocab_size, 128, len(label_cols))\n",
    "#     }\n",
    "\n",
    "#     for name, model in models.items():\n",
    "#         print(f\"\\nTraining model: {name}\")\n",
    "#         criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "#         optimizer = optim.Adam(model.parameters(), lr=0.0008)\n",
    "#         train_model(model, train_loader, criterion, optimizer, epochs=5)\n",
    "\n",
    "#         eval_loader = DataLoader(TensorDataset(train_padded, train_labels), batch_size=64)\n",
    "#         metrics = evaluate_model(model, eval_loader, train_labels)\n",
    "#         print(metrics)\n",
    "#         save_metrics(metrics, f\"{name}_metrics.json\")\n",
    "#         torch.save(model.state_dict(), f\"{name}_model.pth\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de56a0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "✅ Saved predictions to TextCNN_test_predictions.csv\n",
      "✅ Saved predictions to BiLSTM_test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Make sure your model classes are already defined:\n",
    "# TextCNN and BiLSTM (with same hidden_dim etc.)\n",
    "# And also that you have test_padded, test_df, word2idx, etc.\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model hyperparameters must MATCH training\n",
    "emb_dim = 192\n",
    "hidden_dim = 96\n",
    "output_dim = 6  # since label_cols length is 6\n",
    "vocab_size = len(word2idx)  # should be loaded from your earlier code\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "# Prediction + save function\n",
    "def predict_and_save(model_class, model_name, model_file, test_padded, test_df, vocab_size, emb_dim, output_dim, label_cols):\n",
    "    # Step 1: DataLoader\n",
    "    test_loader = DataLoader(test_padded, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Step 2: Load model\n",
    "    model = model_class(vocab_size, emb_dim, output_dim).to(device)\n",
    "    model.load_state_dict(torch.load(model_file, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    # Step 3: Predict\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch_x in test_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            logits = model(batch_x)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > 0.5).float()  # convert to 0/1\n",
    "            all_preds.append(preds.cpu())\n",
    "\n",
    "    # Step 4: Combine predictions\n",
    "    pred_test = torch.cat(all_preds, dim=0).numpy()\n",
    "\n",
    "    # Step 5: DataFrame + save\n",
    "    pred_df = pd.DataFrame(pred_test, columns=label_cols).astype(int)\n",
    "    output_df = pd.concat([test_df['comment_text'].reset_index(drop=True), pred_df], axis=1)\n",
    "\n",
    "    filename = f\"{model_name}_test_predictions.csv\"\n",
    "    output_df.to_csv(filename, index=False)\n",
    "    print(f\"✅ Saved predictions to {filename}\")\n",
    "\n",
    "# --- Run for TextCNN ---\n",
    "predict_and_save(\n",
    "    model_class=TextCNN,\n",
    "    model_name=\"TextCNN\",\n",
    "    model_file=\"TextCNN_model.pth\",\n",
    "    test_padded=test_padded,\n",
    "    test_df=test_df,\n",
    "    vocab_size=vocab_size,\n",
    "    emb_dim=emb_dim,\n",
    "    output_dim=output_dim,\n",
    "    label_cols=label_cols\n",
    ")\n",
    "\n",
    "# --- Run for BiLSTM ---\n",
    "predict_and_save(\n",
    "    model_class=lambda vocab_size, emb_dim, output_dim: BiLSTM(vocab_size, emb_dim, hidden_dim=hidden_dim, output_dim=output_dim),\n",
    "    model_name=\"BiLSTM\",\n",
    "    model_file=\"BiLSTM_model.pth\",\n",
    "    test_padded=test_padded,\n",
    "    test_df=test_df,\n",
    "    vocab_size=vocab_size,\n",
    "    emb_dim=emb_dim,\n",
    "    output_dim=output_dim,\n",
    "    label_cols=label_cols\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b3e824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"word2idx.json\", \"w\") as f:\n",
    "    json.dump(word2idx, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
